import argparse
import asyncio
import json
import os
import sys
import re  # 新增正则表达式模块
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio


class Evaluate():
    def __init__(self, model_name, in_dir, out_dir, max_try, max_workers, base_url, key, eval_model):
        self.prompt_template="""
            [Conversation History]
            {conv_history}

            [Current Round User Prompt]
            {cur_user_prompt}

            [Current Round LLM Response]
            {cur_llm_response}

            [Check List]
            {check_list}

            [Task]
            [Task]
            You are an exceedingly meticulous and fair judge. Your task is to rigorously evaluate whether the [Current Round LLM Response] strictly adheres to every detail specified in the [Current Round User Prompt], using the provided [Check List] as your guide.
            - [Conversation History] provides context from previous rounds of the dialogue.
            - [Current Round User Prompt] represents the latest instruction given by the user in the dialogue; each aspect of this prompt must be addressed with exactness and thoroughness.
            - [Current Round LLM Response] is the response generated by the language model in accordance with the user's prompt; it must meet all explicit and implicit requirements without exception.
            - [Check List] contains specific questions that assess whether the [Current Round LLM Response] meets each detailed requirement outlined in the [Current Round User Prompt]; each item must be scrutinized meticulously.

            For each item in the [Check List], answer with 'Yes' if the criterion is met beyond doubt, or 'No' if there is any deviation, ambiguity, or omission. Provide a clear and concise explanation for your judgment, highlighting how the response does or does not meet the criteria. Justify your answer with reference to both the [Current Round User Prompt] and relevant parts of the [Conversation History].

            **Note**: Some constraints are based on the multi-round dialogue. Please refer to the multi-round dialogue when evaluating, ensuring absolute fidelity to the context and instructions given.
            **Note**: Ensure that all items in [Check List] are rigorously judged, with no omissions and no allowances for partial compliance!
            **Deliverable**: Provide judgement following the designated [Output Format] without including extra analysis or commentary. Any failure to adhere to these instructions should result in a 'No' assessment.

            [Output Format]
            ```json
            {{
                "judge result":[
                    {{
                        "judgement":"<str:only 'Yes' or 'No', indicating whether the constraint was followed.>",
                        "reason":"<str:Provide an explanation for your judgment basis, i.e., the reasoning behind determining whether the constraint was followed>"
                    }},
                    ...
                ]
            }}
            ```
            """
        self.input_file = os.path.join(in_dir, f"{model_name}_infer.json")
        self.output_file = os.path.join(out_dir, f"{model_name}_evaluate.json")
        self.max_try = max_try
        self.client = AsyncOpenAI(
            base_url=base_url,
            api_key=key
            )
        self.max_workers = max_workers
        self.eval_model_name = eval_model

    async def _process_single_turn(self, conv_data, conv_turn_idx, semaphore):
        async with semaphore:
            conv_history = ""
            for idx in range(conv_turn_idx):
                user_prompt = ("user" + ":" + conv_data[idx]["user prompt"] + "\n")
                assistant_ans = ("LLM assistant" + ":" + conv_data[idx]["assistant answer"] + "\n")
                conv_history += (f"c{idx+1}:\n" + user_prompt + assistant_ans)
            conv_history += f'c{conv_turn_idx+1}:\n'
            cur_user_prompt = conv_data[conv_turn_idx]["user prompt"] + "\n"
            cur_llm_response = conv_data[conv_turn_idx]["response"] + "\n"
            check_list = ""
            for check_item in conv_data[conv_turn_idx]["constraints"]:
                check_list += ("- " + check_item['content'] + '\n')
            check_num = len(conv_data[conv_turn_idx]["constraints"])
            
            prompt = self.prompt_template.format(
                conv_history=conv_history,
                cur_user_prompt=cur_user_prompt,
                cur_llm_response=cur_llm_response,
                check_list=check_list
            )
            
            try_time = 0
            while try_time < self.max_try:
                try:
                    response = await self.client.chat.completions.create(
                        model=self.eval_model_name,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=2048,
                        ttemperature=0.01
                    )
                    generated_text = response.choices[0].message.content.strip()
                    
                    json_match = re.search(r'```json\s*({.*?})\s*```', generated_text, re.DOTALL)
                    if not json_match:
                        json_match = re.search(r'({.*})', generated_text, re.DOTALL)
                    
                    if json_match:
                        json_str = json_match.group(1)
                        generated_json = json.loads(json_str)
                        
                        if len(generated_json["judge result"]) == check_num:
                            conv_data[conv_turn_idx]["judge result"] = generated_json["judge result"]
                            return True
                        else:
                            print(f"Retry: Expected {check_num} items, got {len(generated_json['judge result'])}")
                    else:
                        print(f"JSON not found in response: {generated_text}")
                
                except json.JSONDecodeError as e:
                    print(f"JSON decode error: {e}, content: {generated_text}")
                except Exception as e:
                    print(f"API request failed: {e}")
                
                try_time += 1
            
            conv_data[conv_turn_idx]["judge result"] = {"error": f"Failed after {self.max_try} attempts"}
            return False

    async def _process_all_items(self, infer_result):
        tasks = []
        semaphore = asyncio.Semaphore(self.max_workers)
        
        total_turns = sum(len(item["whole_conv"]) for item in infer_result)
        
        for item in infer_result:
            conv_data = item["whole_conv"]
            for conv_turn_idx in range(len(conv_data)):
                task = self._process_single_turn(conv_data, conv_turn_idx, semaphore)
                tasks.append(task)
        
        results = []
        for future in tqdm_asyncio.as_completed(tasks, total=total_turns):
            results.append(await future)
        
        return infer_result

    async def evaluate_async(self):
        with open(self.input_file, 'r', encoding='utf-8') as f:
            infer_result = json.load(f)
        
        result = await self._process_all_items(infer_result)
        
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4)

    def __call__(self):
        if sys.platform == "win32":
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
        asyncio.run(self.evaluate_async())


if __name__=="__main__":
    parser = argparse.ArgumentParser(description="Script for evaluating model responses.")
    parser.add_argument("--key", type=str, default="EMPTY", help="API key for the service.")
    parser.add_argument("--base_url", type=str, default="http://localhost:8902/v1", help="Base URL for the API service.")
    parser.add_argument("--model_name", type=str, default="qwen25-7b-ckpt", help="Name of the model to evaluate.")
    parser.add_argument("--response_dir", type=str, default="./structflow/data", help="Directory to save model responses.")
    parser.add_argument("--eval_dir", type=str, default="./structflow/data/eval", help="Directory to save evaluation results.")
    parser.add_argument("--max_try", type=int, default=2, help="Maximum number of retry attempts.")
    parser.add_argument("--max_workers", type=int, default=16, help="Maximum number of concurrent requests.")
    parser.add_argument("--eval_model", type=str, default="qwen3-32b", help="Model name for evaluation")
    args = parser.parse_args()
    Evaluate(
        model_name=args.model_name,
        in_dir=args.response_dir,
        out_dir=args.eval_dir,
        max_try=args.max_try,
        max_workers=args.max_workers,
        base_url=args.base_url,
        key=args.key,
        eval_model=args.eval_model
    )()